---
layout: page
permalink: /interests/
title: interests
description:
nav: true
nav_order: 2
---
<center>
<div class="image-header">
    <figure>
        <br> 
        <img src='/assets/img/escher/liberation.png' width='95%' title='(inverse?) liberation'>
        <br>
        <span class="figcaption" margin-bottom='2rem' margin-top='-1.5rem' style="color:#763dee"><a href='https://www.wikiart.org/en/m-c-escher/liberation'>'We adore chaos because we love to produce order'</a></span>
        <br>
    </figure>
</div>
</center>

Keywords: <span style="color:#ffffffd1; font-family:monospace; font-size:0.75rem;"><i>computational cognitive science, hyperdimensional computing, vector symbolic algebras, tensor product representations, learning as Bayesian program induction, neural program synthesis, intrinsic curiosity, empiricism vs nativism, embodied learning</i></span>.

My research seeks to develop cognitively-inspired, mathematically-principled deep learning models that illuminate key aspects of human cognition. These efforts are organised around three interconnected research themes:

<span class="font-weight-bold">1. The Paradox of Cognition & Distributed Compositional Architectures</span>: The enduring connectionist/classicalist debate in cognitive science concerns a core challenge: how to reconcile the <i>fluid, probabilistic</i>, and <i>graded</i> dimensions of human cognition with its <i>structured, rule-governed</i>, and <i>compositional</i> aspects? These two cognitive paradigms - one grounded in <b>continuous, statistical processes</b> (<i>the neural</i>), and the other in <b>discrete, algebraic operations</b> (<i>the symbolic</i>) - must ultimately be unified within a single cognitive architecture to account for the full richness of human cognition (see Smolensky’s<a href="https://home.csulb.edu/~cwallis/382/readings/482/smolensky.proper.treat.pdf"> Paradox of Cognition</a>). Inspired by the insights of the <a href="https://www.colorado.edu/ics/sites/default/files/attached-files/92-08.pdf">Integrated Connectionist/Symbolic (ICS) Theory</a> developed by Smolensky, Legendre & Miyata, I have focussed on <b>distributed compositional frameworks</b> - particularly <b>Tensor Product Representations (TPRs)</b> and <b>Vector Symbolic Algebras (VSAs)</b> - as a viable means of resolving this paradox. Grounded in fully continuous mathematics and thus "connectionist all the way down”, these frameworks reliably exhibit <b>emergent symbolic behaviour</b> at higher levels of abstraction due to their carefully orchestrated mathematical design. This phenomenon finds an elegant parallel in physics: while the probabilistic wavefunctions of quantum mechanics provides the most accurate <i>micro-level</i> descsription of reality, these same principles manifest at <i>macroscopic</i> scales as the stable, deterministic laws of classical mechanics. 

In the initial stages of my MPhil, I conducted an extensive investigation of the TPR framework, culminating in the development of the <b>Soft TPR</b> framework (<a href="https://arxiv.org/abs/2412.04671">a first-authored publication at NeurIPS</a>). This work introduces a novel TPR-based compositional representation, while also providing a concrete implementational strategy to learn such representations. Specifically, the Soft TPR framework: 1) <i> formalises and learns flexible, quasi-compositional</i> representations (termed Soft TPRs) that <i>approximate</i> the precise analytical form prescribed by classical TPRs, 2) <i>generates</i> these representations in a <i>single</i> step, thus oblivating the need to <i>token</i> individual constituents to form the compositional representation (once the network is fully trained), and 3) <i>extends</i> TPR-based representation learning to <i>weakly-supervised, non-formal</i> data domains. 

More recently, my attention has turned to <b>VSAs</b>, which retain the benefits of distributed compositionality characteristic of TPRs, while mitigating exponential dimensional growth, enhancing representational flexibility, and offering greater <i>neural plausibility</i>. Taken together, these investigations aim to bridge the gap between the neural and symbolic paradigms of cognition, moving us closer to a <i>truly unified</i> theory of mind.


 <span class="font-weight-bold">2. Bayesian Program Induction for AGI</span>: A second focal point of my research addresses a fundamental question in cognitive science: <i>How do humans, even in infancy, rapidly acquire complex concepts, causal relationships, and intuitive theories about their physical and social environments despite highly limited data, which renders the learning problem statistically underconstrained?</i> The <a href="https://www.jstor.org/stable/188064"> Child-as-Scientist</a> framework provides an illuminating perspective, conceptualising the process of human learning as an <b>active</b> process of theory formation and revision. Of particular interest to me is the <b>Bayesian program induction</b> approach, which formalises theories as <b>probabilistic programs</b> that maximise posterior probability given the available data. Although the use of probabilistic programs confers notable expressive power - capturing causal, counterfactual and explanatory dimensions of cognition - such approaches face significant computational challenges due to the innumerably vast space of possible theories. 
 
 I am interested in <b>scaling</b> computational implementations of child-as-scientist by drawing on insights derived from both <b>empiricist</b> and <b>nativist</b> traditions. First, I am interested in exploring how <b>innate concepts and core knowledge</b> - informed by Elizabeth Spelke's work on domain-specific cognitive structures (e.g., object, cardinality) - can function as strong priors that effectively constrain the hypothesis space, rendering the search for the Bayes optimal hypothesis (theory) more tractable. Second, I am interested in the potential of <b>embodied, egocentric data generation</b>, where active interaction with the environment (modulated by internal goals, states, and evolving hypotheses) adaptively refines priors in real time and directs data collection toward the most relevant portions of the search space. These strategies - though still underexplored - offer promising avenues to overcome the computational intractability that currently limits large-scale implementations of the Child-as-Scientist paradigm. 

<span class="font-weight-bold">3. Integrating Distributed Compositional Frameworks with Deep Learning</span>: A key component of my current research agenda involves <b>translating the mathematical and theoretical formalism</b> of VSAs and TPRs into <b>modern deep learning</b> architectures. This objective stands in contrast to the prevailing 'split-brain' neurosymbolic paradigm in deep learning, wherein neural and symbolic components are isolated into separate modules that communicate via a shared interface. Instead, my aim is to embed the mathematics of these distributed compositional frameworks <b>directly within</b> neural networks, enabling them to harness <b>symbolic-distributed duality</b> by representing symbolic structures as high-dimensional vectors and implementing symbolic operations (e.g., binary tree manipulations) through <b>parallelisable, continuous</b> computations. Several promising directions emerge from this integration:

- <b> Combinatorial Search for Program Synthesis</b>: By leveraging the equivalence between a) symbolic binary trees (i.e., program parse trees) and their VSA encodings, and b) sequential symbolic manipulations of these binary trees (e.g., compositions of car/cdr/cons operations) and parallelisable linear transformations on their corresponding VSA encodings, distributed compositional frameworks - when integrated into neural networks - may <b>mitigate the complexity</b> inherent in searching over combinatorial, discrete program spaces. Additionally, specialised VSA-based optimisation strategies, including <b>resonator networks</b>, hold promise for further enhancing the efficiency of such searches. 
- <b> Neural Abstract Machines </b>: Certain program synthesis approaches exploit the Church-Turing equivalence to induce neural networks to represent algorithmic programs implicitly. However, conventional neural implementations of abstract machines (e.g., Neural Turing Machine, neural pushdown automata), frequently rely on symbolic memory structures, such as matrix-based tapes. Substituting these symbolic structures with VSA encodings not only increases storage capacity - facilitating exponential growth in the number of storable patterns as a function of dimensionaltiy - but may also enhance the <b>stability of gradient-based optimisation</b> and <b>representational flexibility</b>, thereby providing a more robust and scalable pathway for learning implicit programs.
- <b> Compositional Concept Learning </b>: Integrating <b>conceptual role semantics</b> within the VSA framework offers a compelling avenue towards <b>emergent meaning</b>, wherein <b>complex, compositional structures</b> are formed from the randomly initialised, semantically meaningless, high-dimensional seed vectors of the VSA system. As the seed vectors interact systematically to form more compelx structures, the resulting higher-level structures may gradually acquire <b>semantic content</b>, even though the base primitives (the seed vectors) lack intrinsic semantic meaning. 