---
layout: page
permalink: /interests/
title: interests
description:
nav: true
nav_order: 2
---
<center>
<div class="image-header">
    <figure>
        <br> 
        <img src='/assets/img/escher/liberation.png' width='95%' title='(inverse?) liberation'>
        <br>
        <span class="figcaption" margin-bottom='2rem' margin-top='-1.5rem' style="color:#763dee"><a href='https://www.wikiart.org/en/m-c-escher/liberation'>'We adore chaos because we love to produce order'</a></span>
        <br>
    </figure>
</div>
</center>

Keywords: <span style="color:#5a5a5a; font-family:monospace; font-size:0.75rem;"><i>computational cognitive science, hyperdimensional computing, vector symbolic algebras, tensor product representations, learning as Bayesian program induction, neural program synthesis, intrinsic curiosity, empiricism vs nativism, embodied learning</i></span>.

My research seeks to develop cognitively-inspired, mathematically-principled deep learning models that illuminate key aspects of human cognition. Specifically, my interests lie in three interrelated areas: 

<span class="font-weight-bold">1. The Paradox of Cognition & Distributed Compositional Architectures</span>: The enduring connectionist/classicalist debate in cognitive science poses a core challenge: how to reconcile the <i>fluid, probabilistic</i>, and <i>graded</i> dimensions of human cognition with its <i>structured, rule-governed</i>, and <i>compositional</i> aspects. These two paradigms - one grounded in continuous, statistical processes (<i>the neural</i>), and the other in discrete, algebraic operations (<i>the symbolic</i>) - must ultimately be integrated into a single, unified cognitive architecture to fully account for the richness of human cognitive capacities, as highlighted by Smolensky’s<a href="https://home.csulb.edu/~cwallis/382/readings/482/smolensky.proper.treat.pdf"> Paradox of Cognition</a>. Building on the insights of the <a href="https://www.colorado.edu/ics/sites/default/files/attached-files/92-08.pdf">Integrated Connectionist/Symbolic (ICS) Theory</a> of Smolensky, Legendre & Miyata, I have developed a strong interest in <b>distributed compositional frameworks</b>, specifically <b>Tensor Product Representations (TPRs)</b> and <b>Vector Symbolic Algebras (VSAs)</b>, as a promising route to resolving this paradox. Despite being grounded in fully continuous mathematics - remaining “connectionist all the way down” - these frameworks exhibit emergent symbolic behaviour at higher levels of abstraction due to their careful mathematical design. This phenomenon parallels the transition from quantum mechanics, where probabilistic wavefunctions predominate at the <i>micro</i>-level description of reality - to the stable, deterministic laws of classical mechanics, which provides an apt <i>description</i> of those same quantum principles at the <i>macro</i>-level. 

During the initial stages of my MPhil, I undertook an extensive investigation of the TPR framework, culminating in the development of the <b>Soft TPR</b> framework (a first-authored publication at NeurIPS). This work formalises a novel type of TPR-based compositional representation, while also providing a concrete implementational approach. Specifically, the framework: 1) formalises and learns <i>flexible, quasi-compositional</i> representations (termed Soft TPRs) that <i>approximate</i> the precise analytical form prescribed by classical TPRs, 2) generates these representations in a single step, thus oblivating the need to <i>token</i> individual constituents (once the network is fully trained), and 3) <i>extends</i> TPR-based representation learning to <i>weakly-supervised, non-formal</i> data domains. 

More recently, I have turned my attention to <b>VSAs</b>, which retain the advantages of distributed compositionality characteristic of TPRs, while mitigating exponential dimensional growth, and offering enhanced representational flexibility, and strong <i>neural plausibility</i>. Taken together, I hope these efforts can play a part in bridging the gap between the neural and symbolic paradigms of cognition, thereby moving us closer to a truly unified theory of mind.


 <span class="font-weight-bold">2. Bayesian Program Induction for AGI</span>: A second motivator of my research addresses the fundamental mystery of cognitive science: how do humans, even as infants, acquire complex concepts, causal relationships, and intuitive theories about the physical and social world with such sparse input data rendering the learning problem statistically underconstrained? I believe the answer to this lies in the <a href="https://www.jstor.org/stable/188064"> Child as Scientist</a> framework, which conceptualises human learning as an active process of theory formation and theory revision. To this end, I am inspired to build on computational instantiations of child-as-scientist, especially the Bayesian program induction work of Joshua Tenenbaum and colleagues. While theories (hypotheses) are powerful, carrying causal, counterfactual and explanatory weight, a major challenge to computational instantiations of this approach lies in the innumerably vast nature of the space of possible theories. Such a challenge raises profound questions; on a <i>theoretical</i> level: is a fully empiricist approach to learning viable, or even supported by empiricial findings in cognitive science and developmental psychology? On a <i>practical</i> level: how can we efficiently search this space and resolve its computational intractability? To tackle this challenge, I am interested in incorporating cognitive science insights from 2 key areas to scale up the computational instantiation of Bayesian program induction. 1) Innate Concepts and Core Knowledge: Empiricial studies of nativist phenomena, particularly Elizabeth Spelke's work on <i>core knowledge</i> strongly suggest humans possess domain-specific, innate cognitive structures (e.g., object, number) that constrain and scaffold learning. These structures may serve as priors that efficiently prune the hypothesis space, making the search for the Bayes optimal hypothesis (theory) more tractable. 2) Embodiment and Egocentric Data Generation: Active, egocentric interaction with the physical world - shaped by internal states such as goals, aims, and current theories - facilitates the dynamic generation of task-specific data. This dynamic data generating process has significant potential to expedite and guide the search through the hypothesis space by, for instance, refining priors, sharpening likelihoods, and targeting the most relevant regions of the hypothesis space based on the current state of inference. These insights offer promising, yet underexplored avenues for effectively pruning the hypothesis space - a task of utmost importance for computational instantiations of child-as-scientist.

<span class="font-weight-bold">3. Integrating Distributed Compositional Frameworks with Deep Learning</span>: Finally, 