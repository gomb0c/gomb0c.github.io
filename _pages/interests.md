---
layout: page
permalink: /interests/
title: interests
description:
nav: true
nav_order: 2
---
<center>
<div class="image-header">
    <figure>
        <br> 
        <img src='/assets/img/escher/liberation.png' width='95%' title='(inverse?) liberation'>
        <br>
        <span class="figcaption" margin-bottom='2rem' margin-top='-1.5rem' style="color:#763dee"><a href='https://www.wikiart.org/en/m-c-escher/liberation'>'We adore chaos because we love to produce order'</a></span>
        <br>
    </figure>
</div>
</center>

Keywords: <span style="color:#5a5a5a; font-family:monospace; font-size:0.75rem;"><i>computational cognitive science, hyperdimensional computing, vector symbolic algebras, tensor product representations, learning as Bayesian program induction, neural program synthesis, intrinsic curiosity, empiricism vs nativism, embodied learning</i></span>.

My research seeks to develop cognitively-inspired, mathematically-principled deep learning models that illuminate key aspects of human cognition. These efforts are organised around three interconnected research themes.

<span class="font-weight-bold">1. The Paradox of Cognition & Distributed Compositional Architectures</span>: The enduring connectionist/classicalist debate in cognitive science concerns a core challenge: how to reconcile the <i>fluid, probabilistic</i>, and <i>graded</i> dimensions of human cognition with its <i>structured, rule-governed</i>, and <i>compositional</i> aspects. These two cognitive paradigms - one grounded in <b>continuous, statistical processes</b> (<i>the neural</i>), and the other in <b>discrete, algebraic operations</b> (<i>the symbolic</i>) - must ultimately be unified within a single cognitive architecture to account for the richness of human cognitive capacities (see Smolensky’s<a href="https://home.csulb.edu/~cwallis/382/readings/482/smolensky.proper.treat.pdf"> Paradox of Cognition</a>). Building on the insights of the <a href="https://www.colorado.edu/ics/sites/default/files/attached-files/92-08.pdf">Integrated Connectionist/Symbolic (ICS) Theory</a> developed by Smolensky, Legendre & Miyata, I have focussed on <b>distributed compositional frameworks</b> - particularly <b>Tensor Product Representations (TPRs)</b> and <b>Vector Symbolic Algebras (VSAs)</b> - as a viable means of resolving this paradox. Grounded in fully continuous mathematics and thus "connectionist all the way down”, these frameworks reliably exhibit <b>emergent symbolic behaviour</b> at higher levels of abstraction due to their careful mathematical design. This phenomenon parallels the dual description of physical reality - while the probabilistic wavefunctions of quantum mechanics underpin reality and offer its best description at a <i>micro</i>-level, these same principles manifest as the stable, deterministic laws of classical mechanics at a <i>macro</i>-level description of reality. 

In the initial stages of my MPhil, I conducted an extensive investigation of the TPR framework, culminating in the development of the <b>Soft TPR</b> framework (a first-authored publication at NeurIPS). This work introduces a novel TPR-based compositional representation, while also providing a concrete implementational strategy. Specifically, the Soft TPR framework: 1) <i> formalises and learns flexible, quasi-compositional</i> representations (termed Soft TPRs) that <i>approximate</i> the precise analytical form prescribed by classical TPRs, 2) <i>generates</i> these representations in a <i>single</i> step, thus oblivating the need to <i>token</i> individual constituents (once the network is fully trained), and 3) <i>extends</i> TPR-based representation learning to <i>weakly-supervised, non-formal</i> data domains. 

More recently, my attention has turned to <b>VSAs</b>, which retain the benefits of distributed compositionality characteristic of TPRs, while mitigating exponential dimensional growth, enhancing representational flexibility, and offering greater <i>neural plausibility</i>. Taken together, these investigations aim to bridge the gap between the neural and symbolic paradigms of cognition, moving us closer to a <i>truly unified</i> theory of mind.


 <span class="font-weight-bold">2. Bayesian Program Induction for AGI</span>: A second motivator of my research addresses a fundamental mystery of cognitive science: how do humans, even as infants, acquire complex concepts, causal relationships, and intuitive theories about the physical and social world with such sparse input data, which renders the learning problem statistically underconstrained? I believe the answer to this lies in the <a href="https://www.jstor.org/stable/188064"> Child as Scientist</a> framework, which conceptualises human learning as an active process of theory formation and theory revision. To this end, I am inspired to build on computational instantiations of child-as-scientist, especially the Bayesian program induction work of Joshua Tenenbaum and colleagues. While theories (hypotheses) are powerful, carrying causal, counterfactual and explanatory weight, a major challenge to computational instantiations of this approach lies in the innumerably vast nature of the space of possible theories. Such a challenge raises profound questions; on a <i>theoretical</i> level: is a fully empiricist approach to learning viable, or even supported by empiricial findings in cognitive science and developmental psychology? On a <i>practical</i> level: how can we efficiently search this space and resolve its computational intractability? To tackle this challenge, I am interested in incorporating cognitive science insights from 2 key areas to scale up computational implementations of the Bayesian program induction framework. 1) Innate Concepts and Core Knowledge: Empiricial studies of nativist phenomena, particularly Elizabeth Spelke's work on <i>core knowledge</i> strongly suggest humans possess domain-specific, innate cognitive structures (e.g., object, number) that constrain and scaffold learning. These structures may serve as priors that efficiently prune the hypothesis space, making the search for the Bayes optimal hypothesis (theory) more tractable. 2) Embodiment and Egocentric Data Generation: Active, egocentric interaction with the physical world - shaped by internal states such as goals, aims, and current theories - facilitates the dynamic generation of task-specific data. This dynamic data generating process has significant potential to expedite and guide the search through the hypothesis space by, for instance, refining priors, sharpening likelihoods, and targeting the most relevant regions of the hypothesis space based on the current state of inference. These insights offer promising, yet underexplored avenues for effectively pruning the hypothesis space - a task of utmost importance for computational instantiations of child-as-scientist.

<span class="font-weight-bold">3. Integrating Distributed Compositional Frameworks with Deep Learning</span>: Finally, a significant component of my current research agenda focusses on translating the mathematical and theoretical formalism of distributed compositional frameworks - particularly VSAs and TPRs - into modern deep learning architectures. In contrast to the prevalent 'split-brain' neurosymbolic paradigm, which segregates distinctly neural and distinctly symbolic components into discrete, separable modules that communicate across some shared interface, my aim is to integrate the formalism of TPRs/VSAs into neural networks so these networks can harness the symbolic-distributed duality of these frameworks to encode symbolic structures as high-dimensional vectors, and implement symbolic manipulations (e.g., parse-tree transformations, abstract machine operations) using parallelisable, continuous operations. This research agenda opens several promising directions: 

- <b> Combinatorial Search for Program Synthesis</b>: This problem can be reframed by exploiting the equivalence between a) symbolic binary trees (program parse trees) and their VSA-based representations, and b) symbolic manipulations of these binary trees (e.g., compositions of car/cdr/cons operations - which must be applied sequentially) and parallelisable linear maps applied to the symbolic structure's VSA-based representation. This equivalence may be useful in alleviating search complexity in the combinatorial, discrete space of program structures, especially as such a space is often incrementally expanded, in a production-rule-by-production-rule fashion. Additionally, advanced mechanisms, such as resonator networks, could further enhance the efficiency of combinatorial search. 
- <b> Neural Abstract Machines </b>: Conventional neural-net based implementations of abstract machines (e.g., neural turing machine, neural pushdown automata) rely on symbolic representations of structures (e.g., a matrix-based tape). Substituting these symbolic structures with VSA-based representations would improve storage capacity (exponential growth in the number of things that can be stored as a function of $d$), and potentially improve stability of gradient-based training and representational flexibility.
- <b> Compositional Concept Learning </b>: Integrating the conceptual role semantics with the VSA framework offers a viable route to emergent meaning, whereby compositional structures are formed from the randomised, semantically meaningless, high-dimensional seed vectors of the VSA system, and all complex structures acquire semantics through systematic interactions of their constituent parts and the operations that are used in their formation.