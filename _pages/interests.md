---
layout: page
permalink: /interests/
title: interests
description:
nav: true
nav_order: 2
---
<center>
<div class="image-header">
    <figure>
        <br> 
        <img src='/assets/img/escher/liberation.png' width='95%' title='(inverse?) liberation'>
        <br>
        <span class="figcaption" margin-bottom='2rem' margin-top='-1.5rem' style="color:#763dee"><a href='https://www.wikiart.org/en/m-c-escher/liberation'>'We adore chaos because we love to produce order'</a></span>
        <br>
    </figure>
</div>
</center>

Keywords: <span style="color:#5a5a5a; font-family:monospace; font-size:0.75rem;"><i>computational cognitive science, hyperdimensional computing, vector symbolic algebras, tensor product representations, learning as Bayesian program induction, neural program synthesis, intrinsic curiosity, empiricism vs nativism, embodied learning</i></span>.

My research seeks to develop cognitively-inspired, mathematically-principled deep learning models that illuminate key aspects of human cognition. Specifically, my interests lie in three interrelated areas: 

<span class="font-weight-bold">1. The Paradox of Cognition & Distributed Compositional Architectures</span>: The enduring connectionist/classicalist debate in cognitive science poses a core challenge: how to reconcile the <i>fluid, probabilistic</i>, and <i>graded</i> dimensions of human cognition with its <i>structured, rule-governed</i>, and <i>compositional</i> aspects. These two paradigms - one grounded in continuous, statistical processes (<i>the neural</i>), and the other in discrete, algebraic operations (<i>the symbolic</i>) - must ultimately be integrated into a single, unified cognitive architecture to fully account for the richness of human cognitive capacities, as highlighted by Smolensky’s<a href="https://home.csulb.edu/~cwallis/382/readings/482/smolensky.proper.treat.pdf"> Paradox of Cognition</a>. Building on the insights of the <a href="https://www.colorado.edu/ics/sites/default/files/attached-files/92-08.pdf">Integrated Connectionist/Symbolic (ICS) Theory</a> of Smolensky, Legendre & Miyata, I have developed a strong interest in <b>distributed compositional frameworks</b>, specifically <b>Tensor Product Representations (TPRs)</b> and <b>Vector Symbolic Algebras (VSAs)</b>, as a promising route to resolving this paradox. Despite being grounded in fully continuous mathematics - remaining “connectionist all the way down” - these frameworks exhibit emergent symbolic behaviour at higher levels of abstraction due to their careful mathematical design. This phenomenon parallels the transition from quantum mechanics, where probabilistic wavefunctions characterise the <i>micro</i>-level description of reality - to the stable, deterministic laws of classical mechanics, which provides an apt <i>description</i> of those same quantum principles at the <i>macro</i>-level. 

During the initial stages of my MPhil, I undertook an extensive investigation of the TPR framework, culminating in the development of the <b>Soft TPR</b> framework (a first-authored publication at NeurIPS). This work formalises a novel type of TPR-based compositional representation, while also providing a concrete implementational approach. Specifically, the framework: 1) formalises and learns <i>flexible, quasi-compositional</i> representations (termed Soft TPRs) that <i>approximate</i> the precise analytical form prescribed by classical TPRs, 2) generates these representations in a single step, thus oblivating the need to <i>token</i> individual constituents (once the network is fully trained), and 3) <i>extends</i> TPR-based representation learning to <i>weakly-supervised, non-formal</i> data domains. 

More recently, I have turned my attention to <b>VSAs</b>, which retain the advantages of distributed compositionality characteristic of TPRs, while mitigating exponential dimensional growth, in addition to offering enhanced representational flexibility, and strong <i>neural plausibility</i>. Taken together, I hope these efforts can play a part in bridging the gap between the neural and symbolic paradigms of cognition, thereby moving us closer to a truly unified theory of mind.


 <span class="font-weight-bold">2. Bayesian Program Induction for AGI</span>: A second motivator of my research addresses a fundamental mystery of cognitive science: how do humans, even as infants, acquire complex concepts, causal relationships, and intuitive theories about the physical and social world with such sparse input data, which renders the learning problem statistically underconstrained? I believe the answer to this lies in the <a href="https://www.jstor.org/stable/188064"> Child as Scientist</a> framework, which conceptualises human learning as an active process of theory formation and theory revision. To this end, I am inspired to build on computational instantiations of child-as-scientist, especially the Bayesian program induction work of Joshua Tenenbaum and colleagues. While theories (hypotheses) are powerful, carrying causal, counterfactual and explanatory weight, a major challenge to computational instantiations of this approach lies in the innumerably vast nature of the space of possible theories. Such a challenge raises profound questions; on a <i>theoretical</i> level: is a fully empiricist approach to learning viable, or even supported by empiricial findings in cognitive science and developmental psychology? On a <i>practical</i> level: how can we efficiently search this space and resolve its computational intractability? To tackle this challenge, I am interested in incorporating cognitive science insights from 2 key areas to scale up computational implementations of the Bayesian program induction framework. 1) Innate Concepts and Core Knowledge: Empiricial studies of nativist phenomena, particularly Elizabeth Spelke's work on <i>core knowledge</i> strongly suggest humans possess domain-specific, innate cognitive structures (e.g., object, number) that constrain and scaffold learning. These structures may serve as priors that efficiently prune the hypothesis space, making the search for the Bayes optimal hypothesis (theory) more tractable. 2) Embodiment and Egocentric Data Generation: Active, egocentric interaction with the physical world - shaped by internal states such as goals, aims, and current theories - facilitates the dynamic generation of task-specific data. This dynamic data generating process has significant potential to expedite and guide the search through the hypothesis space by, for instance, refining priors, sharpening likelihoods, and targeting the most relevant regions of the hypothesis space based on the current state of inference. These insights offer promising, yet underexplored avenues for effectively pruning the hypothesis space - a task of utmost importance for computational instantiations of child-as-scientist.

<span class="font-weight-bold">3. Integrating Distributed Compositional Frameworks with Deep Learning</span>: Finally, a significant component of my current research agenda focusses on translating the mathematical and theoretical formalism of distributed compositional frameworks - particularly VSAs and TPRs - into modern deep learning architectures. In contrast to the prevalent 'split-brain' neurosymbolic paradigm, which segregates distinctly neural and distinctly symbolic components into discrete, separable modules that communicate across some shared interface, my aim is to integrate the formalism of TPRs/VSAs into neural networks so these networks can harness the symbolic-distributed duality to encode symbolic structures as high-dimensional vectors, and implement symbolic manipulations (e.g., parse-tree transformations, abstract machine operations) using parallelisable, continuous operations. This research agenda opens several promising directions: 

The <b>search challenge in program synthesis</b> can be reframed by exploiting the equivalence between a) symbolic binary trees (program parse trees) and their VSA-based representations, and b) symbolic manipulations of these binary trees (e.g., compositions of car/cdr/cons operations) and parallelisable linear maps applied to their VSA-based representations. This equivalence may be useful to alleviate search complexity in the combinatorial, discrete space of program structures, which is often incrementally expanded, in a production-rule-by-production-rule fashion. Additionally, advanced mechanisms, such as resonator networks, could further enhance the efficiency of combinatorial search. 

