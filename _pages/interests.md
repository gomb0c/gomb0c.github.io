---
layout: page
permalink: /interests/
title: interests
description:
nav: true
nav_order: 2
---
<center>
<div class="image-header">
    <figure>
        <br> 
        <img src='/assets/img/escher/liberation.png' width='95%' title='(inverse?) liberation'>
        <br>
        <span class="figcaption" margin-bottom='2rem' margin-top='-1.5rem' style="color:#763dee"><a href='https://www.wikiart.org/en/m-c-escher/liberation'>'We adore chaos because we love to produce order'</a></span>
        <br>
    </figure>
</div>
</center>

Keywords: <span style="color:#ffffffd1; font-family:monospace; font-size:0.75rem;"><i>hyperdimensional computing, vector symbolic architectures, tensor product representations, learning as Bayesian program induction, program synthesis, empiricism vs nativism, embodied learning, theoretical models of memory</i></span>.

My research seeks to develop cognitively and biologically plausible, mathematically-principled deep learning models. These efforts are currently organised around three interconnected research themes:

<span class="font-weight-bold">1. The Paradox of Cognition & Distributed Compositional Architectures</span>: The enduring connectionist/classicalist debate in cognitive science concerns a core challenge: how to reconcile the <i>fluid, probabilistic</i>, and <i>graded</i> dimensions of human cognition with its <i>structured, rule-governed</i>, and <i>compositional</i> aspects? These two cognitive paradigms - one grounded in <b>continuous, statistical processes</b> (<i>the neural</i>), and the other in <b>discrete, algebraic operations</b> (<i>the symbolic</i>) - must ultimately be unified within a single cognitive architecture to account for the full richness of human cognition (see Smolensky’s<a href="https://home.csulb.edu/~cwallis/382/readings/482/smolensky.proper.treat.pdf"> Paradox of Cognition</a>). Inspired by the insights of the <a href="https://www.colorado.edu/ics/sites/default/files/attached-files/92-08.pdf">Integrated Connectionist/Symbolic (ICS) Theory</a> developed by Smolensky, Legendre & Miyata, I have focussed on <b>distributed compositional frameworks</b> - particularly <b>Tensor Product Representations (TPRs)</b> and <b>Vector Symbolic Architectures (VSAs)</b> - as a viable means of resolving this paradox. Grounded in fully continuous mathematics and thus "connectionist all the way down”, these frameworks reliably exhibit <b>emergent symbolic behaviour</b> at higher levels of abstraction due to their carefully orchestrated mathematical design. This phenomenon finds an elegant parallel in physics: while the probabilistic wavefunctions of quantum mechanics provide the most accurate <i>micro-level</i> description of reality, these same principles can be approximated at <i>macroscopic</i> scales as the stable, deterministic laws of classical mechanics. 

In the initial stages of my MPhil, I conducted an extensive investigation of the TPR framework, which culminated in the development of the <b>Soft TPR</b> framework (<a href="https://arxiv.org/abs/2412.04671">a first-authored publication at NeurIPS</a>). This work introduces a novel TPR-based compositional representation, while also providing a concrete implementational strategy to learn such representations. Specifically, the Soft TPR framework: 1) <i> formalises and learns flexible, quasi-compositional</i> representations (termed Soft TPRs) that <i>approximate</i> the precise analytical form prescribed by classical TPRs, 2) <i>generates</i> these representations in a <i>single</i> step, thus oblivating the need to <i>token</i> individual constituents to form the compositional representation (once the network is fully trained), and 3) <i>extends</i> TPR-based representation learning to <i>weakly-supervised, non-formal</i> data domains. 

More recently, my attention has turned to <b>VSAs</b>, which retain the benefits of distributed compositionality characteristic of TPRs, while mitigating exponential dimensional growth, enhancing representational flexibility, and offering greater <i>neural plausibility</i> compared to their TPR predecessor. Taken together, these investigations aim to bridge the gap between the neural and symbolic paradigms of cognition, moving us closer to a <i>truly unified</i> theory of mind.


 <span class="font-weight-bold">2. Bayesian Program Induction for AGI</span>: A second focal point of my research addresses a fundamental question in cognitive science: <i>How do humans, even in infancy, rapidly acquire complex concepts, causal relationships, and intuitive theories about their physical and social environments despite highly limited data, which renders the learning problem statistically underconstrained?</i> The <a href="https://www.jstor.org/stable/188064"> Child-as-Scientist</a> framework provides an illuminating perspective, conceptualising the process of human learning as an <b>active</b> process of theory formation and revision. Of particular interest to me is the computational instantiation of such a framework provided by the <b>Bayesian program induction</b> approach, which formalises theories as <b>probabilistic programs</b> that maximise posterior probability given the observed data. Although the use of probabilistic programs confers notable expressive power - capturing causal, counterfactual and explanatory dimensions of cognition - such approaches face significant computational challenges due to the innumerably vast space of possible theories. 
 
 I am interested in <b>scaling</b> computational implementations of Child-as-Scientist by drawing on insights derived from both <b>empiricist</b> and <b>nativist</b> traditions in cognitive science. First, I am interested in exploring how <b>innate concepts and core knowledge</b> - informed by Elizabeth Spelke's work on domain-specific cognitive structures (e.g., object, cardinality) - can function as strong priors that effectively constrain the hypothesis space, rendering the search for the Bayes optimal hypothesis (theory) more tractable. Second, I am interested in the potential of <b>embodied, egocentric data generation</b>, where active interaction with the environment (modulated by internal goals, states, and evolving hypotheses) adaptively refines priors in real time and directs data collection toward the most relevant portions of the search space. These strategies offer promising avenues to overcome the computational intractability that currently limits large-scale implementations of the Child-as-Scientist paradigm. 

<span class="font-weight-bold">3. Integrating Distributed Compositional Frameworks with Deep Learning</span>: A key objective of my current research agenda is to <b>translate the mathematical and theoretical formalism</b> of VSAs and TPRs into <b>modern deep learning</b> architectures. By embedding the mathematics of these distributed compositional frameworks <b>directly into</b> neural networks, it becomes possible to <b>encode</b> symbolic structures as high-dimensional vectors and <b>implement</b> symbolic computations (e.g., binary tree manipulations) through <b>parallelisable, continuous</b> operations. This approach holds the promise of <b>preserving interpretability and compositionality</b> - hallmarks of symbolic methods - while benefitting from the <b>scalability, adaptiveness, gradedness, and continuous optimisation strategies</b> intrinsic to distributed systems. Several promising directions naturally follow:

- <b>Combinatorial Search for Program Synthesis</b>: By leveraging the equivalence between a) symbolic binary trees (i.e., program parse trees) and their VSA encodings, and b) sequential symbolic manipulations (e.g., compositions of car/cdr/cons operations) and parallelisable linear transformations on these VSA encodings, distributed compositional frameworks - when integrated into neural networks - may help <b>mitigate the complexity</b> of searching over combinatorial, discrete program spaces. Moreover, specialised VSA-based optimisation strategies, such as <b>resonator networks</b>, could further improve the efficiency of combinatorial search. 
- <b>Neural Abstract Machines </b>: Although some program synthesis approaches exploit the Church-Turing equivalence to induce neural networks capable of representing algorithmic procedures, conventional neural implementations of abstract machines (e.g., Neural Turing Machine, neural pushdown automata) often depend on symbolic memory structures, such as matrix-based memory and symbolic stacks. Replacing these components with VSA-based representations not only expands storage capacity - enabling exponential growth in the number of storable patterns with respect to dimensionality - but may also enhance the <b>stability of gradient-based training</b> and <b>representational flexibility</b>, thus offering a more robust and scalable pathway for learning implicit programs.
- <b>Compositional Concept Learning </b>: An additional avenue of investigation involves the possibility of using an integrated VSA/deep learning system as an implementational framework for conceptual role semantics. Although one might initially regard the randomly initialised, high-dimensional “seed primitives” of VSAs as a limitation - given their lack of intrinsic semantic content - this very characteristic can be viewed differently in light of the conceptual role semantics framework. Specifically, these primitives can be understood as arbitrary states in a dynamical system, such that their systematic combination engenders a network of vector-symbolic structures. Within this network, meaning would thus energe from the functional interrelationships among these seed vectors, rather than from any predefined semantics at the level of individual primitives.

<span class="font-weight-bold">4. Theoretical Models of Memory</span>: I have recently become captivated by theoretical models of memory, including Sparse Distributed Memory and Hopfield Networks. 