---
layout: page
permalink: /interests/
title: interests
description:
nav: true
nav_order: 2
---
<center>
<div class="image-header">
    <figure>
        <br> 
        <img src='/assets/img/escher/liberation.png' width='95%' title='(inverse?) liberation'>
        <br>
        <span class="figcaption" margin-bottom='2rem' margin-top='-1.5rem' style="color:#763dee"><a href='https://www.wikiart.org/en/m-c-escher/liberation'>'We adore chaos because we love to produce order'</a></span>
        <br>
    </figure>
</div>
</center>

Keywords: <span style="color:#5a5a5a; font-family:monospace; font-size:0.75rem;"><i>computational cognitive science, hyperdimensional computing, vector symbolic algebras, tensor product representations, learning as Bayesian program induction, neural program synthesis, intrinsic curiosity, empiricism vs nativism, embodied learning</i></span>.

My research seeks to develop cognitively-inspired, mathematically-principled deep learning models that illuminate key aspects of human cognition. These efforts are organised around three interconnected research themes.

<span class="font-weight-bold">1. The Paradox of Cognition & Distributed Compositional Architectures</span>: The enduring connectionist/classicalist debate in cognitive science concerns a core challenge: how to reconcile the <i>fluid, probabilistic</i>, and <i>graded</i> dimensions of human cognition with its <i>structured, rule-governed</i>, and <i>compositional</i> aspects. These two cognitive paradigms - one grounded in <b>continuous, statistical processes</b> (<i>the neural</i>), and the other in <b>discrete, algebraic operations</b> (<i>the symbolic</i>) - must ultimately be unified within a single cognitive architecture to account for the full richness of human cognition (see Smolensky’s<a href="https://home.csulb.edu/~cwallis/382/readings/482/smolensky.proper.treat.pdf"> Paradox of Cognition</a>). Inspired by the insights of the <a href="https://www.colorado.edu/ics/sites/default/files/attached-files/92-08.pdf">Integrated Connectionist/Symbolic (ICS) Theory</a> developed by Smolensky, Legendre & Miyata, I have focussed on <b>distributed compositional frameworks</b> - particularly <b>Tensor Product Representations (TPRs)</b> and <b>Vector Symbolic Algebras (VSAs)</b> - as a viable means of resolving this paradox. Grounded in fully continuous mathematics and thus "connectionist all the way down”, these frameworks reliably exhibit <b>emergent symbolic behaviour</b> at higher levels of abstraction due to their careful mathematical design. This phenomenon parallels the dual description of physical reality - while the probabilistic wavefunctions of quantum mechanics underpin reality and offer its best description at a <i>micro</i>-level, these same principles manifest as the stable, deterministic laws of classical mechanics at a <i>macro</i>-level description of reality. 

In the initial stages of my MPhil, I conducted an extensive investigation of the TPR framework, culminating in the development of the <b>Soft TPR</b> framework (a first-authored publication at NeurIPS). This work introduces a novel TPR-based compositional representation, while also providing a concrete implementational strategy. Specifically, the Soft TPR framework: 1) <i> formalises and learns flexible, quasi-compositional</i> representations (termed Soft TPRs) that <i>approximate</i> the precise analytical form prescribed by classical TPRs, 2) <i>generates</i> these representations in a <i>single</i> step, thus oblivating the need to <i>token</i> individual constituents (once the network is fully trained), and 3) <i>extends</i> TPR-based representation learning to <i>weakly-supervised, non-formal</i> data domains. 

More recently, my attention has turned to <b>VSAs</b>, which retain the benefits of distributed compositionality characteristic of TPRs, while mitigating exponential dimensional growth, enhancing representational flexibility, and offering greater <i>neural plausibility</i>. Taken together, these investigations aim to bridge the gap between the neural and symbolic paradigms of cognition, moving us closer to a <i>truly unified</i> theory of mind.


 <span class="font-weight-bold">2. Bayesian Program Induction for AGI</span>: A second focal point of my research addresses a fundamental question in cognitive science: <i>How do humans, even as infants, acquire complex concepts, causal relationships, and intuitive theories about the physical and social environments with such limited data, which renders the learning problem statistically underconstrained?</i> The <a href="https://www.jstor.org/stable/188064"> Child as Scientist</a> framework offers a compelling viewpoint, conceptualising human learning as an <b>active</b> process of theory formation and theory revision. I am drawn to computational instantiations of child-as-scientist, especially the <b>Bayesian program induction</b> work of Joshua Tenenbaum and colleagues, which considers the selection of the theory (encoded as a <i>probabilistic program</i>) that has a highest posterior probability. Although encoding theories as probabilistic programs offers expressive power - offering causal, counterfactual and explanatory weight - a major challenge to computational instantiations of this approach lies in the innumerably vast space of possible theories. 
 
 I am interested in <b>scaling</b> computational implementations of child-as-scientist through insights derived from both <b>empiricist</b> and <b>nativist</b> traditions. First, I am interested in exploring how <b>innate concepts and core knowledge</b> - informed by Elizabeth Spelke's work on domain-specific cognitive structures, such as object, cardinality - can serve as strong priors that effectively constrain the hypothesis space, thereby making the search for the Bayes optimal hypothesis (theory) more tractable. Second, I am interested in the potential of <b>embodied, egocentric data generation</b> wherein activate interaction with the environment (modulated by internal goals, states, and evolving hypotheses) can refine these priors in real time and direct data collection toward the most relevant portions of the search space. These approaches - though still underexplored - offer promising avenues to overcome the computational intractability of large-scale computational implementations of child-as-scientist. 

<span class="font-weight-bold">3. Integrating Distributed Compositional Frameworks with Deep Learning</span>: A key component of my current research program involves <b>translating the mathematical and theoretical formalism</b> of VSAs and TPRs into <b>modern deep learning</b> architectures. This undertaking stands in contrast to the prevalent 'split-brain' neurosymbolic paradigm, which isolates neural and symbolic components into distinct modules that communicate via a shared interface. Instead, my aim is to embed these distributed compositional frameworks directly <b>within</b> neural networks, allowing these networks to harness the <b>symbolic-distributed duality</b> by encoding symbolic structures as high-dimensional vectors and enacting symbolic operations (e.g., parse-tree transformations)through  parallelisable, continuous computations. Several promising research directions naturally emerge from this integration:

- <b> Combinatorial Search for Program Synthesis</b>: By exploiting the equivalence between a) symbolic binary trees (program parse trees) and their VSA-based representations, and b) symbolic manipulations of these binary trees (e.g., compositions of car/cdr/cons operations) and parallelisable linear maps on their corresponding VSA encodings, distributed compositional frameworks - when integrated into neural networks - may alleviate the inherent complexity of optimising across large, discrete program spaces. Additionally, advanced mechanisms, including <b>resonator networks</b>, could further improve the efficiency of such combinatorial searches. 
- <b> Neural Abstract Machines </b>: Conventional neural-nework implementations of abstract machines (e.g., Neural Turing Machine, neural pushdown automata) often rely on symbolic memory structures (e.g., a matrix-based tape). Replacing these with VSA-based representations not only increases storage capacity - enabling exponential growth in the number of storable patterns as a function of dimensionaltiy - but may also enhance the <b>stability of gradient-based learning</b> and <b>representational flexibility</b>, thereby providing a more robust and scalable way of learning implicit programs.
- <b> Compositional Concept Learning </b>: Integrating <b>conceptual role semantics</b> within the VSA framework offers a promising pathway to <b>emergent meaning</b>, wherein complex, compositional structures are synthesised from the randomly initialised, semantically meaningless, high-dimensional seed vectors of the VSA system. Under systematic interactions among these seed vectors, and the operations that combine them, complex representations could progressively acquire <b>semantic content</b>.